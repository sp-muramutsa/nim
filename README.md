# ğŸ§  Q-Learning AI Agent in Nim Game
---

## ğŸ“– Project Overview

This project implements a reinforcement learning agent that learns to play the classic game of **Nim** using **Q-learning**, one of the foundational algorithms in model-free **Reinforcement Learning (RL)**.

The AI learns by self-play and gradually builds an optimal strategy through reward-driven experience, using key concepts like:

* **Q-values** for estimating action quality
* **Epsilon-greedy exploration** for balancing learning vs exploiting knowledge
* **Learning rate (Î±)** to control how fast the agent adapts
* **State-action value updates** using temporal-difference learning

---

## ğŸ¯ Core Concepts

| Concept           | Description                                                                |
| ----------------- | -------------------------------------------------------------------------- |
| **State**         | A tuple representing the number of items in each pile.                     |
| **Action**        | A tuple `(i, j)` representing the removal of `j` items from pile `i`.      |
| **Q-value**       | Estimate of expected future reward for a given `(state, action)` pair.     |
| **Î± (alpha)**     | Learning rate controlling how much new info overrides old knowledge.       |
| **Îµ (epsilon)**   | Probability of choosing a random action (exploration vs. exploitation).    |
| **Greedy policy** | Choosing the action with the highest known Q-value (best-so-far strategy). |

---

## ğŸ§  Q-Learning Algorithm

At the heart of the agent is the **Q-learning update rule**:

```
Q(s, a) â† Q(s, a) + Î± [reward + max(Q(s', a')) - Q(s, a)]
```

* `s` is the current state
* `a` is the action taken
* `s'` is the resulting state after the action
* `a'` is the next best action
* `Î±` is the learning rate
* `reward` is +1 (win), -1 (loss), or 0 (intermediate)

- Over thousands of training episodes, the agent refines its Q-values to approximate the optimal strategy.

- The AI uses a **greedy policy** (no exploration) during gameplay for optimal performance.

---

## ğŸ“Š Visual Example

```
PILES
Pile 0: â— 
Pile 1: â— â— â— 
Pile 2: â— â— â— â— â— 
Pile 3: â— â— â— â— â— â— â— 

Your Turn
Choose Pile: 2
Choose Count: 3

AI's Turn
AI chose to take 1 from pile 1.
```

---

## âš™ï¸ No Dependencies

The project uses **only Python standard libraries**:

* `random` for exploration
* `math`, `time` for utility and pacing
* No external packages or ML frameworks

---

## ğŸ§± Project Structure

```
nim_qlearning_ai/
â”œâ”€â”€ nim.py         # Game logic and Q-learning implementation
â”œâ”€â”€ play.py        # Calls the functions for self-play training loop and the agent-vs-human match-up 
â””â”€â”€ README.md      # Project documentation
```

